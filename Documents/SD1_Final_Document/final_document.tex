\documentclass[letterpaper, 12pt]{article}
%\documentclass[letterpaper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[pass,letterpaper]{geometry}
\usepackage{graphicx}
\usepackage{listings}

\setlength{\parindent}{0cm}

\title{Non-Blocking In-Memory Database\thanks{Sponsor: Dr. Damian Dechev}}
\author{Michael McGee, Robert Medina, Neil Moore, Jason Stavrinaky\\[2ex]
	\includegraphics{graphics/OpenMemDB_logo_transparency.png}\\[1ex]
}
\date{11/4/2015}

\pdfinfo{%
  /Title    (Non-blocking, In-memory Database)
  /Author   (Michael McGee, Robert Medina, Neil Moore, Jason Stavrinaky)
  /Creator  (Neil Moore)
  /Producer ()
  /Subject  (Final Document for COP4934)
  /Keywords ()
}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage

\pagenumbering{roman}
\tableofcontents
\newpage

\pagenumbering{arabic}

\section{Executive Summary}
The database as a concept has always emphasized speed, as many other pieces of both software
and business rely on the information contained within them. As such, these pieces of software
are part of a class of software that strive for every piece of performance possible, either through
hardware improvements or algorithmic optimizations. While in-memory databases are not a new concept,
Random Access Memory (RAM) has, until recently, not been cheap enough to completely house a useful
set of data. The ability to do this in such a way beyond simply using main memory as a cache for the 
actual data on the hard disk opens up the possibility of fully utilizing the processing power
available to the system fully.
\par\vspace{\baselineskip}
Utilizing the full hardware resources available to us requires the use of multiple threads or 
multiple processes spread out over all the cores available. Accessing the same data location 
from multiple threads or processes causes issues where threads are unable to progress further in their
work. This state is called deadlock, and is the bane of any programmer that creates multi-threaded
programs. Our project attempts to entirely avoid both deadlock and their common mitigator, locks,
by implementing and using wait-free data structures and algorithms.
\par\vspace{\baselineskip}
Wait-free is a guarantee that the system will always progress, as a whole, within a given period of time
regardless of work-load or contention over resources. The application of this guarantee to many
of the common algorithms and data structures familiar to computer scientists is still under heavy research
(as an example, there is still no widely-accepted implementation of a wait-free binary search tree).
\par\vspace{\baselineskip}
The objectives for this project is to successfully implement a SQL database that is both fully in-memory
and fully wait-free, even at the cost of performance. In order for this to be possible within the time
given to us, the scope of this project must be toned down and strictly enforced. While we will 
attempt to be SQL-compliant, certain aspects of that standard heavily imply a type of mutual 
exclusion be used which we obviously cannot use if we wish to remain wait-free. The pure size of
the standard is also a major obstacle to this objective.
\par\vspace{\baselineskip}
While our technical approach is prone to change as our understanding of the various concepts
and systems at play in a Database Management System (DBMS), we do have a general sense that we
wish to utilize a functional or imperative style of data flow. Objects will be used when appropriate
but we would not describe the overall design as object-oriented in any way. On the contrary, we would 
find a much more suitable term in data-oriented programming as we are almost entirely dealing
with large amounts of arbitrary manipulation rather than the interaction of objects.
\par\vspace{\baselineskip}
To facilitate a wait-free system, we will implement a system where a pre-determined pool of threads
that will be assigned a queue of tasks, whether they be SQL queries or database-specific commands.
These threads will be almost entirely distinct and independent of the others, with as little
inter-communication as possible in order to avoid the possibility of deadlock or the necessity of
locking. The assignment of these tasks will be done by a work manager that will perform some form 
of load-balancing when distributing the tasks among the pool of threads.
\par\vspace{\baselineskip}
Each thread will independently analyze, plan and execute its tasks in such a way that no shared memory
outside of the actual data is needed. The access to the data store, necessary in any database, 
will be handled via a common set of interfaces that will then utilize the algorithms and data structures
given to us by our sponsor. The planning and optimization of these tasks will be primary source 
of technical challenge in this project as the task language chosen (SQL) is declarative rather
than imperative or functional in nature. In other words, the tasks' language only tells us what
to retrieve rather than how to retrieve it.
\par\vspace{\baselineskip}
The advantages given to us by the approach detailed above is the inherent and explicit wait-freedomn
that occurs when the need for locking or shared state is removed entirely. As a result of that, we 
hope to be at least competitive or comparable to current in-memory or traditional databases in terms
of performance.

\newpage

\section{Project Motivation}
As hardware reaches the limits in Moore's Law and processors stop becoming faster and faster
and instead focus on becoming more and more parallel, algorithms and the software that implement
them must adapt in order to maximize both performance and hardware utilization.
\par\vspace{\baselineskip}
% Graphic showing Moore's Law tapering off (graph of GHz and number of cores?)
%\includegraphics{graphics/Moores_Law_core_freq_comparison.png}
Recent research done by multiple universities and companies have yielded the concept of wait-freedom,
the guarantee that the entire system will make progress towards a goal in a given period of time.
This is a stronger guarantee than lock-freedom as lock-freedom only guarantees that a single thread
will make progress in a period of time. Our sponsor, Dr. Dechev, and his lab here at the University
of Central Florida have created a framework of wait-free data structures named Tervel ~\cite{tervel}.

\subsection{Personal Motivations}
\subsubsection{Neil Moore}
This project is one that I knew would be incredibly challenging, yet also had the greatest
potential in terms of personal growth and end product. These technologies are only going to
become more and more important and widespread throughout the software industry as hardware 
continues to parallelize, memory becomes faster and cheaper, and wait-free/lock-free algorithms
become more mature. Experience working with high-performance, memory-intensive, and
standards-defined technologies opens the door for later opportunities in cutting-edge technologies.

\subsubsection{Michael McGee}
  My motivation for choosing this project is simply a desire to learn. I have always been
  interested in database systems as well concurrent programming. Nowhere do these issues
  more converge than in the project that Dr. Dechev proposed. I hope to gain from this
  project, a greater understanding of the underlying principles of database system design,
  as well as discover broader applications for lock-free and wait-free data structures. I
  know that throughout the course of this project I will learn a great deal and come out
  as a much improved software developer and computer scientist.  
\newpage

\section{Broader Impacts}
\newpage

\section{Specification and Requirements}
\newpage

\section{Research}

\subsection{Database Management Systems}
To begin researching a database management system you must first understand what a 
database is. According to the book "Database Management Systems" a database is

\begin{quote}
"... a collection of data, typically describing the activities of one or more related
organizations."\cite{ramakrishnan2000database}
\end{quote}

A database management system, according to the same text is

\begin{quote}
"... software designed to assist in maintaining and utilizing large collections of data".
\cite{ramakrishnan2000database}
\end{quote}

There are many different types of database management system that can achieve the
goal of maintaining large collections of data. Some are specialized for the larger, and 
more volatile web data, while some are more suited for large persistent data.  
\par\vspace{\baselineskip}
Some different types of database management systems are:
\begin{itemize}
\item Relational
\item Hierarchical
\item Network
\item Object-oriented
\item NoSQL
\item NewSQL
\end{itemize}

\subsubsection{Relational}
Perhaps the most common and well known of all database management systems is the
Relational DBMS. The relation data model is based off of tables that represent 
the data, as well as the relationship among the data. It is defined in the book 
"Fundamentals of Relational Database Management Systems" as such:
\begin{quote}
"The relational model uses a collection of tables to represent both data and
the relationships among those data. Tables are logical structures maintained
by the database manager. The relational model is a combination of three
components, such as Structural, Integrity, and Manipulative parts."
\cite{sumathi2007fundamentals}
\end{quote}
\begin{figure}
  \centering
  \textbf{Relational DBMS Structure}
  \includegraphics{graphics/DBMS_Diagrams/dbms_RDBMS_structure.png}
  \caption{Represents a disk based RDBMS}
\end{figure}
The three components can be further broken down.
\par\vspace{\baselineskip}
\textit{The Structural Part} is what defines the database as a collection of relations,
\par\vspace{\baselineskip}
\textit{The Integrity Part} is maintained using primary and foreign keys,
\par\vspace{\baselineskip}and 
\textit{The Manipulative Part} are the tools, such as relational algebra and 
relational calculus, that are used to manipulate the data.
\par\vspace{\baselineskip}
The author lists the key features of the relational model as follows: 
\begin{itemize}
\item Each row in the table is called tuple
\item Each column in the table is called attribute.
\item The intersection of row with the column will have data value.
\item In relational model rows can be in any order.
\item In relational model attributes can be in any order.
\item By definition, all rows in a relation are distinct. No two rows can be exactly
the same.
\item Relations must have a key. Keys can be a set of attributes 
\item For each column of a table there is a set of possible values called its
domain. The domain contains all possible values that can appear under
that column.
\item  Domain is the set of valid values for an attribute.
\item Degree of the relation is the number of attributes (columns) in the relation.
\item Cardinality of the relation is the number of tuples (rows) in the relation.
\end{itemize}

The idea of a \textit{key} is an important one in the world of relational database 
management systems. So it is worth taking some time to talk about it. "A key is an
attribute or a group of attributes, which is used to identify a row in a relation."
\cite{sumathi2007fundamentals}
According to the author of "Fundamentals of Relational Database Management Systems" a
key can be classified into one of three categories.
\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item Superkey
\item Candidate key
\item Primary key
\end{itemize}
A superkey is "a subset of attributes of an entity-set that uniquely identifies
the entities. Superkeys represent a constraint that prevents two entities from
ever having the same value for those attributes."\cite{sumathi2007fundamentals}
\par\vspace{\baselineskip}
A candidate key is "a minimal superkey. A candidate key for a relation schema
is a minimal set of attributes whose values uniquely identify tuples in the
corresponding relation."\cite{sumathi2007fundamentals}
\par\vspace{\baselineskip}
A primary key is a " designated candidate key. It is to be noted that the
primary key should not be null."\cite{sumathi2007fundamentals}
\par\vspace{\baselineskip}
There is also a key known as a foreign key, which is a "set of fields or attributes in one
relation that is used to “refer” to a tuple in another relation."
\cite{sumathi2007fundamentals}
\par\vspace{\baselineskip}
\par\vspace{\baselineskip}
There are many other aspects that full make up what it is to be a relational database
management system. The most important of which being relational algebra. 
\par\vspace{\baselineskip}
"Relational algebra is a theoretical language with operations that work on
one or more relations to define another relation without changing the original
relation"\cite{sumathi2007fundamentals}
Operations of relational algebra include such things as selection operations, projection
operations, rename operations, union operations, intersection operations, difference
operations, division operations, as well as joins. 
\par\vspace{\baselineskip}
The advantages of relational algebra is that is has a solid mathematical background. 
This mathematical background is beneficial when it comes to optimization of queries,
because if two expressions can be proven to be equivalent a query optimizer can 
substitute the more efficient operation whenever necessary.
\par\vspace{\baselineskip}
There are some disadvantages to using a relational model and to relational algebra in
particular. One disadvantage is the growing complexity of the data that needs to be 
stored. Relational database management systems are good for linking together similar types
of data, and with the movement towards increasingly complex data-types these similarities 
are becoming less common. Another disadvantage of an RDBMS is that it can be a complex
system to set up. A database administrator must take significantly more into consideration
when designing and RDBMS then when dealing with a simple object store. 

\subsubsection{Hierarchical}
Hierarchical Databases are a data model in which the data is organized into a tree type 
structure with a one-to-many relationship from the parent to the children. The data 
within the tree are stored as records that are connected together through links. Each
record contains a set of fields with each field containing only one value. 
\par\vspace{\baselineskip}
The hierarchical database model is typically used for large amounts of data that are 
unlikely to change. It is recognized as the first database model used by IBM in 
the 1960s.\cite{hierarchical_dbms_techopedia}
\par\vspace{\baselineskip}
\begin{figure}
  \centering
  \includegraphics{graphics/DBMS_Diagrams/dbms_RDBMS_structure.png}
  \caption{Example structure for hierarchical database}
\end{figure}

\subsubsection{Network}
The Network Model for a database management system is structures similarly to that of the
hierarchical model in that it consists of parent and child nodes. However the network 
model does not limit itself to having every child have only one parent. In the network 
model all children can have multiple parents, and obviously parents can have multiple 
children. The network model also consists of items called "records" that which is a 
collection of data items. Each data item has a name and a value. 

\begin{figure}
  \centering
  \textbf{Network model record example}
  \includegraphics{graphics/DBMS_Diagrams/dbms_network_model_record.gif}
  \caption{Example of network model record}
  \cite{network_model_coronet}
\end{figure}

Each record can have it's own grouping within by grouping two or more elementary items.
A record can also contain a table, which is a collection of values that are grouped 
under one data item. 
\par\vspace{\baselineskip}
The data within the network model consists of two main parts: data objects, and
relationships.
 
\begin{figure}
  \centering
  \includegraphics{graphics/DBMS_Diagrams/dbms_network_data.gif}
  \caption{Network information model}
  \cite{network_model_coronet}
\end{figure}

Relationships between records can be implemented by using logical constructions. 
This is called a "Data Set" . In the most basic case, each set consists of a father 
and a child. The "Data Set" has the following properties: 

\begin{itemize}
  \item Each set includes exactly one record of the first type. This record is called an Owner of the set.
  \item Each set may include 0 (i.e. an Empty set occurrence), 1 or N records of the same type. These records are called members of the data set.
  \item All members within one set occurrence have a fixed order (are sorted).
\end{itemize}

\begin{figure}
  \centering
  \includegraphics{graphics/network/relationship.gif}
  \caption{Example of network model relationship}
\end{figure}

The basic setup of a network model is therefore a collection of record occurrences and
data sets. 

\subsubsection{Object-oriented}
Object oriented databases are databases where the information stored within is organized
in the form of objects, like what would be used in an object oriented programming
language. An OODBMS combines the the features of an object-oriented language and a
DBMS. The OODBMS permits a much tighter coupling between the database and the application. This allows the programmer to maintain consistency within a single environment. 
\par\vspace{\baselineskip}
In an OODBMS data is encapsulated in \textit{abstract data objects}, also known 
as ADOs. ADOs all have the following properties: 

\begin{itemize}
  \item It has a unique identity
  \item It has a private memory and a number of operations that can be applied to the current state of that memory. 
  \item The values held in the private memory are themselves ADOs that are referenced from
within by means of variable identifiers called instance variables. Note the emphasis
“from within”, which underlines the idea of encapsulation, ie. such instance variables
or objects they denote or any organisation of the objects into any structure in the
private memory are not visible from outside the ADO.
  \item The only way that the internal state of an ADO can be accessed or modified from
outside is through the invocation of operations it provides. An operation can be
invoked by sending a message to it. The message must of course contain enough
information to decide which operation to invoke and provide also any input needed
by that operation. The object can respond to the message in a number of ways, but
typically by returning some (other) object back to the message sender and/or causing
some observable change (eg. in a graphical user interface).
\end{itemize} \cite{object_oriented_data_model}

\begin{figure}
  \centering
  \includegraphics{graphics/DBMS_Diagrams/dbms_oodbms_example.png}
  \caption{An example of an object-oriented model}
\end{figure}

\subsubsection{NoSQL}

\subsubsection{NewSQL}

\newpage

\subsection{Work Manager}
The Work Manager is a conglomeration of multiple minor, but not insignificant, roles
within the DBMS. Specifically, it is the:
\begin{itemize}
  \item Main thread
  \item Sole communicator with the client
  \item Load-balancer between the primary worker threads
  \item System resource monitor
\end{itemize}

Work distribution between threads is done via a task-queue that each thread in the
thread pool pulls work from. Tasks are C++11 lambda functions that are packaged up 
via the standard library's \lstinline[basicstyle=\ttfamily]|std::packaged_task<>|
 class. 
This class provides an interface that allows us to generate a \lstinline[basicstyle=\ttfamily]|std::future<>| 
object that is a thread-safe interface to a thread's return value. The bulk of the 
ideas behind this thread pool were found online \cite{stackoverflow1} though we substituted
the command queue, that was initially implemented using a \lstinline[basicstyle=\ttfamily]|std::mutex|
and \lstinline[basicstyle=\ttfamily]|std::queue<>|, with a Tervel queue that allows
non-blocking and wait-free access and insertion into the command queue.
\par\vspace{\baselineskip}
The task of communicating between the client and the server, which is the database itself,
is given to the work manager as it is the main thread and is where all the results of
the SQL query end up. This task also requires initialization and management of the network
sockets as well as the management of the socket identification numbers that are assigned
to specific connections. In turn, the module that manages these connections must be able to
determine which connection a query came from and where the query's results should be
returned to. The module that is in the best place for this work is the work manager
and has therefore been assigned these tasks.
\par\vspace{\baselineskip}
The work manager is also the part of the database management system that will be cognizant
of the amount of resources used by the system. Processor utilization, memory usage, thread
performance, and various other metrics listed below will be measured and able to be
requested by clients or management software. The full list of system resources measured
and reported by the server is as follows:
\begin{itemize}
 \item Processor utilization
 \item Amount of memory in use
 \item Percentage of RAM used in comparison to the maximum capacity of the hardware
 \item Shortest execution time of a query
 \item Longest execution time of a query
 \item Median execution time of a query
 \item Average ``load'' of a worker thread
 \item TODO: others??
\end{itemize}

\subsection{Data Store}
\subsubsection{Indexing}
%author Robert Medina
Indexing allows for fast retrieval from a collection of data. There are many ways to accomplish this, and some ways are better suited depending on the constraints. Tree-based indexing and Hash-based indexing are two popular solutions for an implementation of a database. Given a set of data in memory, indexing takes a key data value and stores it into a data structure. Based off this key value, the indexing data structure will point to the file in memory or memory address, depending on how the data is stored. Based on this, a file scan of the database is now just reduced to scanning an index file. 
%\includegraphics{graphics/indexentry.png}
However searching through an index file can still be a costly operation. Index files are smaller than the data it is referencing but it can still use a considerable about memory space. Therefore it would be reasonable to use a data structure for fast retrieval of data based on a range of values or based on the actual data value stored.


\subsubsection{Tree-based Indexing}
%author Robert Medina
Tree-based indexing is an index file structured into a variant of a binary search tree. The tree is based off the key value and references to where that key is stored. Indexing requires fast retrieval of data, low cost insertion and low cost deletion of data. There are two types of trees that are useful for this type of operation, ISAM and B+ Tree.

\subsubsection{B+ Tree}
%author Robert Medina
B+ Trees are a variant of a B Tree. B Trees is an n-ary tree that splits data
	based on a key value into nodes with T(j) subtrees and K(j) nodes, where j = (1<j<m).
	By definition a B tree is a tree with the following properties:
	Each node consists of T(i) subtrees and K(i) keys, where i = (0,...,j)
	There is a single root that contains a range of m children, 2<= m < j
	Each node contains a range of m children, (m/2) <= m < j
	For each sub-tree T(i) where i = (0,...,j) each sub-tree contains keys k(j) such that:
		T(0) sub-tree have keys k(0) where k(0) < k(i)
		T(i) sub-tree have keys k(i), i = (1, ..., j-1) where k(0) <= k(i) <= k(j)
		T(j) sub-tree have keys k(j) where k(j) < k(j)
	All T(i) sub-trees are either non-empty or empty.
	B Trees have a height of log(m)(N), m = number of children
%\includegraphics{graphics/B_Tree_Example.png}
B+ Tree differs slightly in the fact that B trees store their data (keys) within the internal nodes while B+ trees only points to the data. All the data in a B+ is stored in the leaves of tree. In addition, B+ Tree links all their leaves together with a doubly linked list.
%\includegraphics{graphics/B_B+_Tree_Difference.png}
The runtime for a B+ Tree is the following:
Searching is logm(n), m is index entries
Insertion is same as searching
Deletion is same as searching
Searching with range (logm(n) + k), m is index entries and k is number of data records
Key Compression:
The height of a B+ tree is log(m)(n), m = number of children, n = number of data entries.
Tradiationally for databases on disk the number of data entries would be based on the size of the data and
the size of the page. This is because tree nodes should fit on a single page, if B+ Tree leaves take up more memory than a page offers then B+ tree is resized to fit onto a page. Since the page can only fit so much data this is why the number of data entries is dependent on the size of data entries. So, it is reasonable to max the number of data entries by making the size of data smaller. This enures that more entires will fit on a page and the height of the B+ tree would remain small, thus keeping data retrevial performance fast.
\subsubsection{Hash-based Indexing}
%author Robert Medina
	Hash-based Indexing references a key value to a pointer using a hash function. This can be used for indexing when there is an equal key value within the hash. Range operations using a key value is not possible with a hash function since it would simply be too costly of an operation. Hash-based indexing suffers from overflow chaining such as ISAM, which can hinder performance. There exists multiple hash-based indexing such as Static Hashing, Extendible Hashing, and Linear Hashing. 
\subsubsection{Static Hashing}
%author Robert Medina
Static Hashing is a hash table based on a key value with a pointer to a bucket of pages that contain said key. These data entries may be sorted but it depends on the application. This data structure is static for the most part but it does allow for overflow page allocation. In the event of inserting beyond the memory allocated, this data entry is placed into a new page and the page is added to an overflow chain in the hash table.
%\includegraphics{graphics/Static_Hashing_Abstract}
A good hash function is imperative to uniformly distribute values over the collection of buckets. An example of a good hash would be hash(key) = (a*key + b), a and b are constants. Some problems of Static Hashing are the fact that it is static. When the index file is created bucket sizes are known at time of creation, so pages can be stored successively in the buckets. However as the index file continues to grow if the same key value is stored repeatedly then a long overflow chain develops. Since the number of buckets are static if the index file shrinks in size then there is wasted memory space. If the file grows too large then it results in poor performance. Otherwise, the performance for operations is very fast. The following is the runtimes:
Search O() 1 I/O read
Insertion O() 2 I/O read/write page
Deletion O() 2 I/O read/write page
Rehashing:
	Intuitively a simple hash table with a pointer to a page would make sense. However in the case of additional pages, without an overflow chain rehashing the table would be necessary. In this case rehashing the table would be a costly operation. Including that the data structure is unusable while rehashing is in progress. Dynamic hashing techniques solve this problem.
Extendible Hashing:

\subsection{SQL Engine}
%author Mike McGee
One of the most important pieces of a Database Management System is the SQL Engine. 
This component is responsible for receiving commands written in the standard query
language and transforming those commands into an internal representation that can be
executed by the DBMS. The SQL Engine often consists of three pieces: the tokenizer, the
parser, and the code generator.

%\includegraphics{graphics/SQLEngine_activity_overview.png}

\subsubsection{Tokenizer}
%initial author Neil Moore
To start the query planning and execution process, we must tokenize the provided query
from a straight string into a format readable by the parser. This tokenizer is very
similar to a generic compiler tokenizer though it's job is made considerably easier 
by the importance of spaces in SQL queries/commands regarding token delineation.
\par\vspace{\baselineskip}
In an effort to reduce code duplication and facilitate rapid development, we chose
to appropriate the tokenizer from the SQLite project. SQLite is a public domain
SQL database library that integrates very cleanly into many applications big
or small.
\par\vspace{\baselineskip}
SQLite is not entirely SQL compliant, as that is simply unreasonable to achieve
given its use cases and problem domain. As such, changes to the tokenizer will be
needed in order to make sure it can handle proper SQL queries/commands while 
not supporting non-standard SQL tokens.

\subsubsection{Parser}
%author Mike McGee
When researching SQL parsers we found that most database management systems use a
parser generator tool to develop a parser for the query language that they support.
The purpose of any parser generator is to implement a parser in the programming
language desired that will accurately parse the context free grammar that it 
was passed in the grammar specification file.
\par\vspace{\baselineskip}
The two parser generators that we found were "Lemon" and "YACC". PostgreSQL uses 
"YACC", which stands for "Yet Another Compiler Compiler", while SQLite uses 
"Lemon", which stands for "Lemon". Both tools will generate a C code parser for 
your query language when provided a grammar specification file. According to the 
tutorial provided by "Lemon" there are some vast differences between these 
two parser generators. 
\par\vspace{\baselineskip}
“It uses a different grammar syntax which is designed to reduce the number of coding
errors. Lemon also uses a more sophisticated parsing engine that is faster than yacc and
bison and which is both reentrant and thread-safe. Furthermore, Lemon implements features
that can be used to eliminate resource leaks, making is suitable for use in long-running
programs such as graphical user interfaces or embedded controllers.”\cite{lemon_parser}
\par\vspace{\baselineskip}
It is because of these benefits, especially thread safety, that we chose to use Lemon
as our parser generator. The Lemon parser generator is contained in one C code file and
is used by running the program with the grammar specification file as an argument.
This terminal command would resemble \textit{lemon gram.y} 
Upon completion Lemon will produce between one and three files.\\ Those files are:
\begin{itemize}
	\item \textit{gram.c}: C code implementation of the parser
	\item \textit{gram.h}: A header file defining an integer ID for each terminal sybmol
	\item \textit{gram.out}: An information file that describes the states of the
	generated parser automaton
\end{itemize}
Lemon does not generate a complete program, it only creates a few subroutines that
implement a parser. It is up to the developer to call those subroutines in an appropriate
way in order to produce a complete system. In order to use a Lemon generated parser the
developer must first create the parser as follows: 
\begin{lstlisting} 
	void *pParser = ParseAlloc( malloc );
\end{lstlisting}
This call allocates and initializes a new parser and returns a pointer to it. The
parameter to the call is the subroutine used to allocate memory. For our purposes it will
most likely be something Tervel specific.

After the programmer is done using the parser they must free the memory that was allocated
to the parser using a subroutine of their choice. It is done as follows
\begin{lstlisting}
	ParseFree (pParser, free) 
\end{lstlisting}
where free is the subroutine used to reclaim the memory, again probably Tervel specific for our purposes. 

After the parser is allocated, the developer will provide the parser with a sequence of
tokens to be parsed. This is done by calling:
\begin{lstlisting}
	Parse(pParser, hTokenID, sTokenData, pArg);
\end{lstlisting}

\begin{quotation}
“The first argument to the Parse() routine is the pointer returned by ParseAlloc(). The
second argument is a small positive integer that tells the parse the type of the next
token in the data stream. There is one token type for each terminal symbol in the grammar.
The gram.h file generated by Lemon contains \#define statements that map symbolic terminal
symbol names into appropriate integer values. (A value of 0 for the second argument is a
special flag to the parser to indicate that the end of input has been reached.) The third
argument is the value of the given token. By default, the type of the third argument is
integer, but the grammar will usually redefine this type to be some kind of structure.
Typically the second argument will be a broad category of tokens such as ``identifier'' or
``number'' and the third argument will be the name of the identifier or the value of the
number. The Parse() function may have either three or four arguments, depending on the
grammar. If the grammar specification file request it, the Parse() function will have a
fourth parameter that can be of any type chosen by the programmer. The parser doesn't do
anything with this argument except to pass it through to action routines. This is a
convenient mechanism for passing state information down to the action routines without
having to use global variables.”\cite{lemon_parser}
\end{quotation}


\subsubsection{Query Planner}
The query planner is the most challenging and important piece of a typical DBMS as it
determines how the data in the SQL tables are retrieved which has a huge impact on 
the overall performance of the DBMS. The need for a query planner comes from the 
declarative nature of SQL queries, where the query doesn't tell you how to retrieve the
data but instead tells you what to retrieve. This is similar to languages such as Prolog where
a theorem solver is employed to determine how to solve the given problem. A large theorem 
solver is impractical in a system with high-performance, low-latency environment such as 
a DBMS, so we must generate our own significantly stripped-down version that can handle a 
specific set of query relations extremely fast.

\subsubsection{Code Generator}

\subsection{Related Works}
	Over the last 30 years the there has been tremendous advancements in computing
	hardware. "Processors are thousands of times faster and memory is thousands of
	times times larger"\cite{stonebraker2007end}. Most technologies have advanced 
	along with hardware, however database management systems have struggled to improve
	at a similar rate. This is mostly due to concurrency issues. "Existing studies show
	that current database engines can spend more than 30\% of time in 
	synchronization-related operations (e.g.locking and latching), even when only a 
	single client thread is running."\cite{soares2015database}
   \par\vspace{\baselineskip}
  	There have been several attempts to solve this problem. Some of which will sacrifice
  	some data consistency in order to achieve a higher better performance. Still others
  	remain fully ACID compliant and attempt to parallelize individual steps in the 
  	DBMS or solely use multiple threads when executing query plans. Then there are those
  	that attempt to implement some level of lock freedom into their DBMS.
  	\par\vspace{\baselineskip}
	\subsubsection{MemSQL and VoltDB}
	MemSQL\footnote{MemSQL can be configured as a Columnstore that stores data on disk}
	and VoltDB are both fully in memory DBMS as is OpenMemDB. This is where the
	similarities end as far as OpenMemDB is concerned. MemSQL and VoltDB on the other 
	hand both use distributed systems to achieve performance gains. MemSQL differs from 
	VoltDB in a few ways, the most important being it's use of lock free data structures
	for storing data and its storing of pre-compiled commonly used queries\cite{MemSQL}.
	\begin{figure}
	  \centering
	  \textbf{MemSQL Two-tiered Architecture}
	  \includegraphics{graphics/RelatedWorksArchitectures/memsql_dbms_architecture.png}
	  \caption{MemSQL Architecture}
	\end{figure}
	VoltDB tries to make its performance gains by what they call "Concurreny through
	scheduling"\cite{VoltDB}. This is the process of using a single-threaded pipeline 
	that performs the task it was scheduled. This limits the need for locking during
	transactions by intelligently scheduling the transactions so that locks are not
	necessary.
	\begin{figure}
	  \centering
	  \textbf{VoltDB Serialized Architecture}
	  \includegraphics{graphics/RelatedWorksArchitectures/voltdb_serialized_architecture.png}
	  \caption{VoltDB Serialized Processing}
	\end{figure}
	\par\vspace{\baselineskip}
	OpenMemDB aims to take a different approach, one that is fully wait free. The goal is 
	to use powerful wait free data structures that will allow for a massively parallel 
	DBMS that can scale with the addition of processors and memory. It is our assumption 
	that the achievement of a fully weight free DBMS will achieve the performance gains 
	that have been lacking in the DBMS world while eliminating the complexity of 
	distributed systems, all while retaining full ACID compliance.
	
	\subsubsection{Other Database Management Systems}
	The two DBMS systems listed above are far from the only ones that exist. They were
	researched more heavily because of there perceived similarities to our project and
	because the documentation for them was thorough. However, there are numerous lesser
	known and less well documented DBMS that are worthy of note.

	\begin{itemize}
	  \par\vspace{\baselineskip}
	  \item Aerospike is an AGPL licensed NoSQL flash-optimized in memory 
	  DBMS. Aerospike is an open source project that follows the similar pattern of 
	  a distributed shared-nothing architecture that is appearing common among most
	  in-memory DBMS. 
	  \begin{quote}
	  "Aerospike architecture is derived from its core principles – NoSQL scalability and
	  flexibility, along with traditional database consistency, reliability and
	  extensibility."
	  \cite{aerospike}
	  \end{quote}
	  \par\vspace{\baselineskip}
	  \begin{figure}
	    \centering
	    \includegraphics{graphics/RelatedWorksArchitectures/aerospike_dbms_architecture.png}
	    \caption{Aerospike Architecture}
	  \end{figure}
	  \cite{aerospike}
	  \par\vspace{\baselineskip}	
	  \item Apache Geode is an Apache licensed distributed in-memory DBMS. Apache Geode is
	  a brand new project and as such has very limited documentation. They claim on there
	  github page to be 
	  \begin{quote}
	  "... a data management platform that provides real-time, consistent access to 
	  data-intensive applications throughout widely distributed cloud architectures."
	  \cite{aerospike}
	  \end{quote}
	  The specifics of their architecture is not listed. 
	  \par\vspace{\baselineskip}
	  \item dashDB is IBM's in-memory data warehouse. Self described as 
	  \begin{quote}
	  "a high performance, massively scalable cloud data warehouse service, 
	  fully managed by IBM." \cite{dashDB}
	  \end{quote}
	  dashDB comes in a couple different configurations, the one that most resembles 
	  our project is the MPP, "Massively Parallel Processing", configuration. 
	  MPP operates by allowing the data warehouse to leverage multiple servers 
	  in a network cluster to process data simultaneously. 
	  \par\vspace{\baselineskip}
%%	\includegraphics{graphics/RelatedWorksArchitectures/dashDB_mpp_architecture.png}
	  \cite{dashDB}
	  \par\vspace{\baselineskip}
	  Each server in this data warehouse utilizes a number of key technologies, including: 
	  \begin{itemize}
	    \item Dynamic in-memory processing: Even when a dataset
	    does not fit entirely in memory, dashDB still processes at
	    lightning fast speeds using a series of patented algorithms
	    that enable in-memory acceleration. While every workload
	    is different, dashDB only requires RAM size to be 5 percent
	    of the original pre-load source data size in order to run at
        in-memory optimized speeds.
        \item Actionable compression: dashDB performs a broad range
	    of operations—including joins and predicate evaluations—
	    directly on compressed data, therefore improving memory
	    and cache bandwidth, and saving CPU costs.
   	   \item Parallel vector processing: dashDB is CPU optimized
	   and designed for the latest generation of microprocessors.
	   Both multi-core parallelism and SIMD vector instructions
       enable dashDB to maximize hardware performance.
	   \item Data skipping: BLU enables dashDB to intelligently avoid
	   scanning entire ranges of column data that don’t qualify for
	   analysis, preserving time and resources.
	 \end{itemize}
	
	  dashDB uses a highly parallelized infrastructure optimized for columnar data
	  exchange that is organized as such:
	  \par\vspace{\baselineskip}
%%	\includegraphics{graphics/RelatedWorksArchitectures/dashDB_query_architecture.png}
	  \cite{dashDB}
	  \par\vspace{\baselineskip}
	  \item eXtremeDB is the in-memory variant of the McObject family of data management
	  projects.
	  It stores its data entirely in main memory, eliminating the need for disk I/O. 
	  eXtremeDB claims to have an "Ultra-small" footprint stating that through 
	  streamlining of core database functions they are able to reduce their RAM footprint
	  to around 100KB. They also do not translate the data they store in memory. They 
	  store the data exactly how it will be used by the application. "No mapping a C data
	  element to a relational representation"\cite{extremeDB} eXtremeDB claims to fully 
	  support ACID properties. It does this by ensuring that operations grouped into
	  transactions will complete together or the database will be rolled back to a 
	  pre-transaction state.\cite{extremeDB}
	  \par\vspace{\baselineskip}
%%	\includegraphics{graphics/RelatedWorksArchitectures/extremeDB_dbms_architecture.jpg}
	  \cite{extremeDB}
	  \par\vspace{\baselineskip}
	  
	  \item GemFire is a distributed, in-memory, shared-nothing, NoSQL key-value store.
	  GemFire is designed for working with operational data needed by real time 
	  applications. It is not meant for working on very large quantities. In order to 
	  achieve massive speeds GemFire relies on being primarily, not fully, 
	  main memory based. " It uses highly-concurrent main-memory data structures to avoid
	  lock contention and a data distribution
	  layer that avoids redundant message copying, and it uses native serialization and
	  smart buffering to ensure messages move from node to node faster than what
	  traditional messaging would provide."\cite{gemfire}
	  \par\vspace{\baselineskip}
%%	  \includegraphics{graphics/RelatedWorksArchitectures/gemfire_peer_architecture.png}
	  \par\vspace{\baselineskip}
	  
	  \item Hekaton is Microsoft's database engine that is optimized for memory resident
	  OLTP data. Hekaton is fully implemented within SQL Server and can be utilized from
	  within. 
	  \begin{quote}
	  "Hekaton is designed for high levels of concurrency but does not
	  rely on partitioning to achieve this. Any thread can access any row
	  in a table without acquiring latches or locks. The engine uses latchfree
	  (lock-free) data structures to avoid physical interference
	  among threads and a new optimistic, multiversion concurrency
	  control technique to avoid interference among transactions"\cite{hekaton}
	  \end{quote}
	  Hekaton stores it's data in two different formats: a lock-free hash table and a 
	  lock free B-tree called a Bw-tree. Data is always accessed with an index lookup.
	  \par\vspace{\baselineskip}
	  Hekaton's Architecture:
	  \begin{itemize}
	    \item The Hekaton storage engine manages user data and indexes.
		It provides transactional operations on tables of records, hash
		and range indexes on the tables, and base mechanisms for storage,
		checkpointing, recovery and high-availability.
		\item The Hekaton compiler takes an abstract tree representation of
		a T-SQL stored procedure, including the queries within it, plus
		table and index metadata and compiles the procedure into native
		code designed to execute against tables and indexes managed
		by the Hekaton storage engine.
		\item The Hekaton runtime system is a relatively small component
		that provides integration with SQL Server resources and
		serves as a common library of additional functionality needed
		by compiled stored procedures.
		\item Metadata: Metadata about Hekaton tables, indexes, etc. is
		stored in the regular SQL Server catalog. Users view and manage
		them using exactly the same tools as regular tables and
		indexes.
		\item Query optimization: Queries embedded in compiled stored
		procedures are optimized using the regular SQL Server optimizer.
		The Hekaton compiler compiles the query plan into native
		code.
		\item Query interop: Hekaton provides operators for accessing data
		in Hekaton tables that can be used in interpreted SQL Server
		query plans. There is also an operator for inserting, deleting,
		and updating data in Hekaton tables.
		\item Transactions: A regular SQL Server transaction can access
		and update data both in regular tables and Hekaton tables.
		Commits and aborts are fully coordinated across the two engines.
		\item High availability: Hekaton is integrated with AlwaysOn,
		SQL Server’s high availability feature. Hekaton tables in a database
		fail over in the same way as other tables and are also
		readable on secondary servers.
		\item Storage, log: Hekaton logs its updates to the regular SQL
		Server transaction log. It uses SQL Server file streams for storing
		checkpoints. Hekaton tables are automatically recovered
		when a database is recovered. 
	  \end{itemize} \cite{hekaton}
	  
	  \item SAP HANA
	  is short for "High Performance Analytic Appliance" and is an in-memory,
	  column-oriented, relational database management system. SAP HANA employs a
	  massively parallel in-memory architecture in order to eliminate the I/O bottleneck
	  that slows disk backed database management systems.
	  \par\vspace{\baselineskip} 
%%	  \includegraphics{graphics/RelatedWorksArchitectures/sap_hana_dbms_architecture.png}
	  \cite{saphana}
	  \par\vspace{\baselineskip}
	\end{itemize}	
	
\newpage

\section{Design Summary}
\newpage

\section{Facilities and Equipment}
Our primary meeting place as a team was the computer science senior design lab in Harris Engineering
Center (HEC) 105. Workstations are provided along with collaboration equipment such as 
whiteboards and computers hooked up to TVs. A rack-mounted server was also made available and 
some members of the team were given virtual machines on it to use as development
platforms.
\par\vspace{\baselineskip}
Individual group members were able to use their personal machines as development platforms, though 
eventually all but the most trivial testing had to be moved to the server(s) we had access to
as their consumer-level hardware could not provide enough raw power to properly benchmark and
stress-test the system. The hardware used by each member:
\begin{itemize}
 \item Thor (Neil Moore's personal machine)
 \begin{itemize}
  \item{\makebox[4cm]{CPU:\hfill} AMD FX-8350 (8-core)}
  \item{\makebox[4cm]{RAM:\hfill} 16 GB DDR3}
  \item{\makebox[4cm]{Motherboard:\hfill} Asus M5A99X EVO R2.0}
  \item{\makebox[4cm]{GPU:\hfill} AMD Radeon HD 7870}
  \item{\makebox[4cm]{Primary Hard Disk:\hfill} PNY Optima 240 GB SSD}
  \item{\makebox[4cm]{Total Available Storage:\hfill} 3 terabytes}
  \item{\makebox[4cm]{Operating System:\hfill} Arch Linux}
 \end{itemize}
\end{itemize}
\par\vspace{\baselineskip}
In addition to the senior design lab, we also had access to the Scalable and Secure Systems lab server
thanks to Dr. Dechev. This server was the primary test-bed for OpenMemDB as consumer hardware simply
doesn't have the necessary parallization or memory needed in order to truly test and stress a
highly-parallelized and memory-intensive database management system. The server we were allowed to 
use had the following hardware characteristics:
\begin{itemize}
 \item 64 cores
 \item 312 gigabytes of RAM
 \item 1 terabyte of hard disk storage
\end{itemize}

\newpage

\subsection{Consultants, Subcontractors, and Suppliers}
Throughout the course of designing and developing this database management system we sought the advice
and knowledge of various people, as none of us are experts or even particularly knowledgeable in 
massively parallel systems. Our primary source of information and advice was Steven Feldman, the 
original developer of Tervel and its maintainer, even past his departure from the University of Central
Florida to Google. His insight into the internals of Tervel proved invaluable, as did his many 
explanations on the concepts of wait-freedom and how to create a wait-free system.
\par\vspace{\baselineskip}
Dr. Dechev also proved a reliable source to consult when either Steven was unavailable or when he
had particular insight into a problem we were facing. 
%Include stuff about the other guy...
\newpage

\section{Budget and Financing}
Thanks to the contributions from our sponsor and the nature of our project, we did not need to expend
any additional funds outside of normal maintenance each team member performed on their personal
machines.
\newpage

\section{Project Summary}
\newpage

\bibliography{final_document}
\bibliographystyle{acm}
\newpage

\appendix
\section{Appendix A}


\end{document}
