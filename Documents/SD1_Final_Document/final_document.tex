\documentclass[letterpaper]{article}
%\documentclass[letterpaper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[pass,letterpaper]{geometry}
\usepackage{graphicx}
\usepackage{listings}

\title{Non-Blocking In Memory Database}
\author{}
\date{}

\pdfinfo{%
  /Title    (Non-blocking, In-memory Database)
  /Author   (Michael McGee, Robert Medina, Neil Moore, Jason Stavrinaky)
  /Creator  (Neil Moore)
  /Producer ()
  /Subject  (Initial Project Description for COP4934)
  /Keywords ()
}

\begin{document}
\maketitle
\newpage

\tableofcontents
\newpage

\section{Executive Summary}
\newpage

\section{Project Motivation}

\subsection{Personal Motivations}
\newpage

\section{Broader Impacts}
\newpage

\section{Specification and Requirements}
\newpage

\section{Research}
\subsection{Indexing}
%author Robert Medina
Indexing allows for fast retrieval from a collection of data. There are many ways to accomplish this, and some ways are better suited depending on the constraints. Tree-based indexing and Hash-based indexing are two popular solutions for an implementation of a database. Given a set of data in memory, indexing takes a key data value and stores it into a data structure. Based off this key value, the indexing data structure will point to the file in memory or memory address, depending on how the data is stored. Based on this, a file scan of the database is now just reduced to scanning an index file. 
\includegraphics{graphics/indexentry.png}
However searching through an index file can still be a costly operation. Index files are smaller than the data it is referencing but it can still use a considerable about memory space. Therefore it would be reasonable to use a data structure for fast retrieval of data based on a range of values or based on the actual data value stored.


\subsection{Tree-based Indexing}
%author Robert Medina
Tree-based indexing is an index file structured into a variant of a binary search tree. The tree is based off the key value and references to where that key is stored. Indexing requires fast retrieval of data, low cost insertion and low cost deletion of data. There are two types of trees that are useful for this type of operation, ISAM and B+ Tree.

\subsection{B+ Tree}
%author Robert Medina
B+ Tree is a balanced tree in which the non-leaf nodes direct where the data will be stored and the leaf nodes contain the data entries. The leaf nodes are contained in a doubly-linked list for fast retrieval of all leaf nodes.
\includegraphics{graphics/B+_Tree_Abstract.png}
Specifically, B+ Tree is an n-ary tree that has a root, internal nodes and leaf children. Each index entry contains an array of key values and a pointer. Each node references to another node based on a branching property. Each parent node contains m+1 children, m is index entries. The left children must be less than the parent key value, the middle children must be between the left and right side children values, and the right children must be greater or equal to the parent key value. Every node except root must be at least half full. 
\includegraphics{graphics/B+_Tree_Example.png}
The runtime for a B+ Tree is the following:
Searching is logm(n), m is index entries
Insertion is same as searching
Deletion is same as searching
Searching with range (logm(n) + k), m is index entries and k is number of data records

\subsection{Hash-based Indexing}
%author Robert Medina
	Hash-based Indexing references a key value to a pointer using a hash function. This can be used for indexing when there is an equal key value within the hash. Range operations using a key value is not possible with a hash function since it would simply be too costly of an operation. Hash-based indexing suffers from overflow chaining such as ISAM, which can hinder performance. There exists multiple hash-based indexing such as Static Hashing, Extendible Hashing, and Linear Hashing. 
\subsection{Static Hashing}
%author Robert Medina
Static Hashing is a hash table based on a key value with a pointer to a bucket of pages that contain said key. These data entries may be sorted but it depends on the application. This data structure is static for the most part but it does allow for overflow page allocation. In the event of inserting beyond the memory allocated, this data entry is placed into a new page and the page is added to an overflow chain in the hash table.
\includegraphics{graphics/Static_Hashing_Abstract}
A good hash function is imperative to uniformly distribute values over the collection of buckets. An example of a good hash would be hash(key) = (a*key + b), a and b are constants. Some problems of Static Hashing are the fact that it is static. When the index file is created bucket sizes are known at time of creation, so pages can be stored successively in the buckets. However as the index file continues to grow if the same key value is stored repeatedly then a long overflow chain develops. Since the number of buckets are static if the index file shrinks in size then there is wasted memory space. If the file grows too large then it results in poor performance. Otherwise, the performance for operations is very fast. The following is the runtimes:
Search O() 1 I/O read
Insertion O() 2 I/O read/write page
Deletion O() 2 I/O read/write page
Rehashing:
	Intuitively a simple hash table with a pointer to a page would make sense. However in the case of additional pages, without an overflow chain rehashing the table would be necessary. In this case rehashing the table would be a costly operation. Including that the data structure is unusable while rehashing is in progress. Dynamic hashing techniques solve this problem.
Extendible Hashing:

\subsection{SQL Engine}
%author Mike McGee
One of the most important pieces of a Database Management System is the SQL Engine. 
This component is responsible for receiving commands written in the standard query
language and transforming those commands into an internal representation that can be
executed by the DBMS. The SQL Engine often consists of three pieces: the tokenizer, the
parser, and the code generator.

%Insert an image here that shows the sql engine process

\subsubsection{Tokenizer}

\subsubsection{Parser}
%author Mike McGee
When researching SQL parsers we found that most database management systems use a 
parser generator tool to develop a parser for the query language that they support.
The purpose of any parser generator is to implement a parser in the programming language
desired that will accurately parse the context free grammar that it was passed in the 
grammar specification file.
\\\\
The two parser generators that we found were "Lemon" and "YACC". PostgreSQL uses "YACC",
which stands for "Yet Another Compiler Compiler", while SQLite uses "Lemon", which stands
for "Lemon". Both tools will generate a C code parser for your query language when
provided a grammar specification file. According to the tutorial provided by "Lemon" there
are some vast differences between these two parser generators. 
\\
“It uses a different grammar syntax which is designed to reduce the number of coding
errors. Lemon also uses a more sophisticated parsing engine that is faster than yacc and
bison and which is both reentrant and thread-safe. Furthermore, Lemon implements features
that can be used to eliminate resource leaks, making is suitable for use in long-running
programs such as graphical user interfaces or embedded controllers.”\cite{lemon_parser}
\\\\
It is because of these benefits, especially thread safety, that we chose to use Lemon
as our parser generator. The Lemon parser generator is contained in one C code file and
is used by running the program with the grammar specification file as an argument.
This terminal command would resemble \textit{lemon gram.y} 
Upon completion Lemon will produce between one and three files.\\ Those files are:
\begin{itemize}
	\item \textit{gram.c}: C code implementation of the parser
	\item \textit{gram.h}: A header file defining an integer ID for each terminal sybmol
	\item \textit{gram.out}: An information file that describes the states of the
	generated parser automaton
\end{itemize}
Lemon does not generate a complete program, it only creates a few subroutines that
implement a parser. It is up to the developer to call those subroutines in an appropriate
way in order to produce a complete system. In order to use a Lemon generated parser the
developer must first create the parser as follows: 
\begin{lstlisting} 
	void *pParser = ParseAlloc( malloc );
\end{lstlisting}
This call allocates and initializes a new parser and returns a pointer to it. The
parameter to the call is the subroutine used to allocate memory. For our purposes it will
most likely be something Tervel specific.
\\\\ 
After the programmer is done using the parser they must free the memory that was allocated
to the parser using a subroutine of their choice. It is done as follows
\begin{lstlisting}
	ParseFree (pParser, free) 
\end{lstlisting}
where free is the subroutine used to reclaim the memory, again probably Tervel specific for our purposes. 
\\\\
After the parser is allocated, the developer will provide the parser with a sequence of
tokens to be parsed. This is done by calling:
\begin{lstlisting}
	Parse(pParser, hTokenID, sTokenData, pArg);
\end{lstlisting}
“The first argument to the Parse() routine is the pointer returned by ParseAlloc(). The
second argument is a small positive integer that tells the parse the type of the next
token in the data stream. There is one token type for each terminal symbol in the grammar.
The gram.h file generated by Lemon contains \#define statements that map symbolic terminal
symbol names into appropriate integer values. (A value of 0 for the second argument is a
special flag to the parser to indicate that the end of input has been reached.) The third
argument is the value of the given token. By default, the type of the third argument is
integer, but the grammar will usually redefine this type to be some kind of structure.
Typically the second argument will be a broad category of tokens such as ``identifier'' or
``number'' and the third argument will be the name of the identifier or the value of the
number. The Parse() function may have either three or four arguments, depending on the
grammar. If the grammar specification file request it, the Parse() function will have a
fourth parameter that can be of any type chosen by the programmer. The parser doesn't do
anything with this argument except to pass it through to action routines. This is a
convenient mechanism for passing state information down to the action routines without
having to use global variables.”\cite{lemon_parser}

\subsubsection{Code Generator}

\newpage

\section{Design Summary}
\newpage

\section{Facilities and Equipment}
\newpage

\section{Consultants, Subcontractors, and Suppliers}
\newpage

\section{Budget and Financing}
\newpage

\section{Project Summary}
\newpage

\bibliography{final_document}
\bibliographystyle{acm}
\newpage

\appendix
\section{Appendix A}


\end{document}
